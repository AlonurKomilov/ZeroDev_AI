from abc import ABC, abstractmethod
from typing import Any, Dict, List

from backend.core.logger import get_logger
from backend.core.openai_client import client
from backend.core.settings import settings

log = get_logger(__name__)


class LLMAdapter(ABC):
    """Abstract Base Class for Large Language Model adapters."""

    @abstractmethod
    async def chat_completion(
        self, messages: List[Dict[str, str]], **kwargs: Any
    ) -> Dict[str, Any]:
        """
        Generates a chat completion response from the language model.

        :param messages: A list of message dictionaries (e.g., [{"role": "user", "content": "Hello"}]).
        :param kwargs: Additional keyword arguments for the model provider (e.g., model, temperature).
        :return: A dictionary representing the model's response, ideally in a standardized format.
        """
        pass


class OpenAIAdapter(LLMAdapter):
    """Adapter for OpenAI's models (e.g., GPT-3.5, GPT-4)."""

    def __init__(self):
        """
        Initializes the adapter and validates the OpenAI API key.
        """
        if (
            not settings.OPENAI_API_KEY
            or settings.OPENAI_API_KEY == "your_openai_api_key_here"
        ):
            raise ValueError(
                "❌ OPENAI_API_KEY is not configured. Please set it in your .env file or environment variables."
            )

    async def chat_completion(
        self, messages: List[Dict[str, str]], **kwargs: Any
    ) -> Dict[str, Any]:
        model = kwargs.get("model", "gpt-4o-mini")
        log.debug(f"Using OpenAIAdapter with model: {model}")
        try:
            # Ensure the model is passed to the client
            kwargs["model"] = model
            response = await client.chat.completions.create(messages=messages, **kwargs)
            # Return the response as a dictionary for consistent handling
            return response.model_dump()
        except Exception as e:
            log.error(f"OpenAI API error with model {model}: {e}", exc_info=True)
            raise  # Re-raise the exception to be handled by the caller


class ClaudeMockAdapter(LLMAdapter):
    """A mock adapter for Anthropic's Claude models for demonstration purposes."""

    async def chat_completion(
        self, messages: List[Dict[str, str]], **kwargs: Any
    ) -> Dict[str, Any]:
        model = kwargs.get("model", "claude-3-opus-mock")
        log.debug(f"Using ClaudeMockAdapter with model: {model}")

        # This is a mock response that mimics the structure of a real API call.
        # It's useful for testing the router without making a real API call.
        mock_response = {
            "id": f"mock-claude-resp-{hash(messages[-1]['content'])}",
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": '{"project_name": "Mock Project", "description": "A mock project generated by the Claude adapter.", "targets": ["mock_target_1", "mock_target_2"]}',
                    },
                    "finish_reason": "stop",
                    "index": 0,
                }
            ],
            "model": model,
            "usage": {"prompt_tokens": 25, "completion_tokens": 50, "total_tokens": 75},
        }
        return mock_response


def get_llm_adapter(model_name: str) -> LLMAdapter:
    """
    Factory function that returns an appropriate LLM adapter based on the model name.
    This acts as the central router for selecting the AI provider.
    """
    log.info(f"Routing request for model: {model_name}")
    if model_name.lower().startswith("gpt"):
        return OpenAIAdapter()
    elif model_name.lower().startswith("claude"):
        log.warning(f"Routing to mock adapter for Claude model: {model_name}")
        return ClaudeMockAdapter()
    else:
        log.error(
            f"No adapter found for model '{model_name}'. Defaulting to OpenAIAdapter."
        )
        # Defaulting to OpenAI is a safe fallback.
        return OpenAIAdapter()
